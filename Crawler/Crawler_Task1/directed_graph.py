from crawler import Crawler
from main import *
from file_handler import *

dic={}
links=[]
crawled_file="crawled.txt""
links= file_list()
#create a dictionary with crawled links.

dic = dic.fromkeys(links.split[-1],"")
#for i in range(0,len(Crawler.crawled)):
#    links.append(Crawler.crawled.split[-1])


def fetcher():
    depth=1
    Crawler.temp.append(HOMEPAGE)
   while depth <= 5 :
        cr.boot()


